<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="A training-free method to improve the perception capability of Vision-Language Models.">
  <meta property="og:title" content="Aligning What Vision-Language Models See and Perceive with Adaptive Information Flow"/>
  <meta property="og:description" content="A training-free method to improve the perception capability of Vision-Language Models."/>
  <meta property="og:url" content="https://dreamix-video-editing.github.io"/>
  <meta property="og:image" content="static/image/og_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Aligning What Vision-Language Models See and Perceive with Adaptive Information Flow">
  <meta name="twitter:description" content="A training-free method to improve the perception capability of Vision-Language Models.">
  <meta name="twitter:image" content="static/images/twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Vision-language models, adaptive information flow, visual perception">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Aligning What Vision-Language Models See and Perceive with Adaptive Information Flow</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Aligning What Vision-Language Models See and Perceive with Adaptive Information Flow</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
             <span class="author-block">
              <a href="https://cxliu0.github.io/" target="_blank">Chengxin Liu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://wons20k.github.io/" target="_blank">Wonseok Choi</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://chenshuang-zhang.github.io/" target="_blank">Chenshuang Zhang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://ami.kaist.ac.kr/members/tae-hyun-oh" target="_blank">Tae-Hyun Oh</a><sup>1</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>KAIST,</span>
              <span class="author-block"><sup>2</sup>POSTECH</span>              
            </div>
            
            <div class="publication-venue">            
              <span class="is-size-5"><strong>CVPR 2026</strong></span>                     
          </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://cxliu0.github.io/AIF/" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper (TBU)</span>
                  </a>
                </span>


            <!-- ArXiv abstract Link -->
            <span class="link-block">
              <a href="https://cxliu0.github.io/AIF/" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv (TBU)</span>
              </a>
            </span>

          </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-Language Models (VLMs) have demonstrated strong capability in a wide range of tasks such as visual recognition, document parsing, and visual grounding. 
            Nevertheless, recent works show that while VLMs often manage to capture the correct image region corresponding to the question, they do not necessarily produce the correct answers. 
            In this work, we demonstrate that this misalignment could be attributed to suboptimal information flow within VLMs, where text tokens distribute too much attention to irrelevant visual tokens, leading to incorrect answers. 
            Based on the observation, we show that modulating the information flow during inference can improve the perception capability of VLMs. 
            The idea is that text tokens should only be associated with important visual tokens during decoding, eliminating the interference of irrelevant regions. 
            To achieve this, we propose a token dynamics-based method to determine the importance of visual tokens, where visual tokens that exhibit distinct activation patterns during different decoding stages are viewed as important. 
            We apply our approach to representative open-source VLMs and evaluate on various datasets including visual question answering, visual grounding and counting, optical character recognition, and object hallucination. 
            The results show that our approach significantly improves the performance of baselines.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Key idea -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Key Idea</h2>
          <center>
          <img src="static/images/figure1.png" alt="Key Idea" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              <b>Test-time information flow modulation improves perception.</b> (a) VLMs often manage to capture related object regions with
              text-to-image attention (e.g., Mickey Mouse clock in the input image), but sometimes fail to answer the question correctly. 
              (b) We observe that visual tokens corresponding to object regions show distinct activation pattern in certain
              layers of the language model. In contrast, visual tokens in distracting regions exhibit irregular activation pattern across different layers.
              (c) Based on (b), we observe that disconnecting the interaction between text tokens and distracting visual tokens during inference lead
              to improved results, enabling the model to correctly select Mickey Mouse for question answering. This information flow modulation is
              implemented by modulating causal mask and applied during attention computation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End key idea -->


<!-- Method -->
<section class="section is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Method Overview</h2>
          <center>
          <img src="static/images/pipeline.png" alt="Method Overview" class="center-image"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Given an input image and user prompt, one-step language decoding is applied to obtain (b) token
              dynamics. Then, token dynamics-based entropy is computed, resulting in a token entropy map. Subsequently, (c) adaptive token masking is
              adopted for causal mask generation. The generated causal mask is fed to the language model for reasoning, achieving test-time information
              flow modulation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End method -->


<section class="section is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Qualitative Results</h2>
          <center>
          <img src="static/images/visualization.png" alt="Qualitative Results" class="center-image"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Visualizations of masked tokens and text-to-image attention after information flow modulation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{liu2026aif,
  title={Aligning What Vision-Language Models See and Perceive with Adaptive Information Flow},
  author={Chengxin, Liu and Wonseok, Choi and Chenshuang, Zhang and Tae-Hyun, Oh},
  booktitle={CVPR},
  year={2026}
}</code></pre>
  </div>
</section>
<!-- End BibTex citation -->
  
  
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. 
            This website is licensed under a <a rel="license"  href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- Default Statcounter code for Dreamix
https://dreamix-video-editing.github.io -->
<script type="text/javascript">
var sc_project=12843789; 
var sc_invisible=1; 
var sc_security="e9c3bf5f"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12843789/0/e9c3bf5f/1/"
alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>

<!-- End of Statcounter Code -->

</body>
</html>
